{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9f4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d2c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\"\n",
    "merged_file_path=r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Ranked Stocks\"\n",
    "\n",
    "files = [\n",
    "    \"PE_Annual.csv\",\n",
    "    \"PB_Annual.csv\",\n",
    "    \"DividendYield_Data.csv\",\n",
    "    \"MarketCap_Data.csv\",\n",
    "    \"ROE_Annual.csv\",\n",
    "    \"ROA_Annual.csv\",\n",
    "    \"Volume_data.csv\",\n",
    "    \"DebtToEquity_Annual.csv\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2875f9a",
   "metadata": {},
   "source": [
    "First we are merging all the files of parameters together into one single csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec127782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Ranked Stocks\\final_merged_by_year.csv\n",
      "Final merged columns: ['Ticker', 'StockPrice_x', 'NetEPS', 'PE', 'Year', 'StockPrice_y', 'BookValuePerShare', 'PB', 'DividendYield', 'MarketCap', 'ROE', 'NetIncome', 'ROA', 'TotalAssets', 'Volume', 'DebtToEquity']\n",
      "  Ticker  Year\n",
      "0   AAPL  2010\n",
      "1   AAPL  2011\n",
      "2   AAPL  2012\n",
      "3   AAPL  2013\n",
      "4   AAPL  2014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in files:\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Ensure Ticker exists\n",
    "    if 'Ticker' not in df.columns:\n",
    "        raise KeyError(f\"'Ticker' column missing in {filename}\")\n",
    "    \n",
    "    # Normalize Year\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = df['Year'].astype(int)\n",
    "    elif 'Date' in df.columns:\n",
    "        df['Year'] = pd.to_datetime(df['Date'], errors='coerce').dt.year\n",
    "        df.drop(columns=['Date'], inplace=True)\n",
    "    else:\n",
    "        raise KeyError(f\"'{filename}' must have either 'Date' or 'Year' column.\")\n",
    "\n",
    "    df['Ticker'] = df['Ticker'].astype(str).str.strip()\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=['Ticker', 'Year'], how='outer')\n",
    "\n",
    "output_path = os.path.join(merged_file_path, \"final_merged_by_year.csv\")\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged file saved to: {output_path}\")\n",
    "\n",
    "print(\"Final merged columns:\", merged_df.columns.tolist())\n",
    "print(merged_df[['Ticker', 'Year']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b90620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final ranked file saved to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Ranked Stocks\\final_scored_ranked.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_rolling_percentile(df, col, window=5):\n",
    "    return df.groupby('Ticker')[col].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).apply(\n",
    "            lambda s: pd.Series.rank(s).iloc[-1] / len(s)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Calculate rolling percentiles\n",
    "merged_df['PE_percentile'] = 1 - calculate_rolling_percentile(merged_df, 'PE')\n",
    "merged_df['PB_percentile'] = 1 - calculate_rolling_percentile(merged_df, 'PB')\n",
    "merged_df['ROE_percentile'] = calculate_rolling_percentile(merged_df, 'ROE')\n",
    "merged_df['ROA_percentile'] = calculate_rolling_percentile(merged_df, 'ROA')\n",
    "merged_df['DebtToEquity_percentile'] = 1 - calculate_rolling_percentile(merged_df, 'DebtToEquity')\n",
    "\n",
    "# Combine into category scores\n",
    "merged_df['Value_Score'] = (merged_df['PE_percentile'] + merged_df['PB_percentile']) / 2\n",
    "merged_df['Quality_Score'] = (merged_df['ROE_percentile'] + merged_df['ROA_percentile']) / 2\n",
    "merged_df['Risk_Score'] = merged_df['DebtToEquity_percentile']\n",
    "\n",
    "# Total weighted score\n",
    "merged_df['Total_Score'] = (\n",
    "    0.4 * merged_df['Value_Score'] +\n",
    "    0.4 * merged_df['Quality_Score'] +\n",
    "    0.2 * merged_df['Risk_Score']\n",
    ")\n",
    "\n",
    "# Make sure 'Year' column exists\n",
    "if 'Year' not in merged_df.columns:\n",
    "    if 'Date' in merged_df.columns:\n",
    "        merged_df['Year'] = pd.to_datetime(merged_df['Date'], errors='coerce').dt.year\n",
    "    else:\n",
    "        raise ValueError(\"No 'Year' or 'Date' column to extract Year from.\")\n",
    "\n",
    "# Filter stepwise by Year (example: keep top 150 by ROE, then top 100 by ROA, etc.)\n",
    "def stepwise_filtering(df, year, filters):\n",
    "    yearly_df = df[df['Year'] == year].copy()\n",
    "    for col, ascending, n in filters:\n",
    "        yearly_df = yearly_df.sort_values(col, ascending=ascending).head(n)\n",
    "    return yearly_df\n",
    "\n",
    "# Define your filtering sequence (example)\n",
    "filters = [\n",
    "    ('ROE', False, 150),       # highest ROE keep top 150\n",
    "    ('ROA', False, 100),       # highest ROA keep top 100\n",
    "    ('DividendYield', False, 80), # highest DividendYield keep top 80\n",
    "    ('DebtToEquity', True, 60),   # lowest DebtToEquity keep top 60\n",
    "    ('PE', True, 50),          # lowest PE keep top 50\n",
    "    ('PB', True, 40),          # lowest PB keep top 40\n",
    "    ('MarketCap', False, 30),  # highest MarketCap keep top 30\n",
    "    ('Volume', False, 20)      # highest Volume keep top 20\n",
    "]\n",
    "\n",
    "# Run filtering & collect filtered data for each year\n",
    "filtered_list = []\n",
    "for year in merged_df['Year'].unique():\n",
    "    filtered_year_df = stepwise_filtering(merged_df, year, filters)\n",
    "    filtered_list.append(filtered_year_df)\n",
    "\n",
    "filtered_df = pd.concat(filtered_list)\n",
    "\n",
    "# Now rank filtered stocks by Total_Score per year\n",
    "filtered_df['Rank'] = filtered_df.groupby('Year')['Total_Score'].rank(ascending=False, method='dense')\n",
    "\n",
    "# Save final ranked stocks per year\n",
    "final_cols = ['Ticker', 'Year', 'PE', 'PB', 'ROE', 'ROA', 'DebtToEquity',\n",
    "              'DividendYield', 'MarketCap', 'Volume',\n",
    "              'Value_Score', 'Quality_Score', 'Risk_Score', 'Total_Score', 'Rank']\n",
    "\n",
    "output_path = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Ranked Stocks\\final_scored_ranked.csv\"\n",
    "filtered_df[final_cols].to_csv(output_path, index=False)\n",
    "print(f\"✅ Final ranked file saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae027c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peratio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
