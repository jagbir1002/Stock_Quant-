{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef58c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac49a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NVDA\n",
      "Processing: MSFT\n",
      "Processing: AAPL\n",
      "Processing: AMZN\n",
      "Processing: GOOGL\n",
      "Processing: META\n",
      "Processing: AVGO\n",
      "Processing: TSLA\n",
      "Processing: TSM\n",
      "Processing: BRK-B\n",
      "Processing: WMT\n",
      "Processing: JPM\n",
      "Processing: ORCL\n",
      "Processing: V\n",
      "Processing: LLY\n",
      "Processing: MA\n",
      "Processing: NFLX\n",
      "Processing: XOM\n",
      "Processing: COST\n",
      "Processing: PLTR\n",
      "Processing: JNJ\n",
      "Processing: HD\n",
      "Processing: PG\n",
      "Processing: ABBV\n",
      "Processing: SAP\n",
      "Processing: BAC\n",
      "Processing: CVX\n",
      "Processing: KO\n",
      "Processing: GE\n",
      "Processing: AMD\n",
      "Processing: CSCO\n",
      "Processing: ASML\n",
      "Processing: TMUS\n",
      "Processing: BABA\n",
      "Processing: PM\n",
      "Processing: WFC\n",
      "Processing: TM\n",
      "Processing: IBM\n",
      "Processing: CRM\n",
      "Processing: ABT\n",
      "Processing: AZN\n",
      "Processing: MS\n",
      "Processing: NVS\n",
      "Processing: UNH\n",
      "Processing: LIN\n",
      "Processing: MCD\n",
      "Processing: HSBC\n",
      "Processing: NVO\n",
      "Processing: INTU\n",
      "Processing: SHEL\n",
      "Processing: RTX\n",
      "Processing: GS\n",
      "Processing: BX\n",
      "Processing: AXP\n",
      "Processing: DIS\n",
      "Processing: MRK\n",
      "Processing: T\n",
      "Processing: PEP\n",
      "Processing: SHOP\n",
      "Processing: CAT\n",
      "Processing: UBER\n",
      "Processing: RY\n",
      "Processing: NOW\n",
      "Processing: VZ\n",
      "Processing: BKNG\n",
      "Processing: GEV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GEV: possibly delisted; no price data found  (1d 2010-01-01 -> 2023-12-31) (Yahoo error = \"Data doesn't exist for startDate = 1262322000, endDate = 1703998800\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] GEV: 'Dividends'\n",
      "Processing: HDB\n",
      "Processing: ANET\n",
      "Processing: SCHW\n",
      "Processing: BLK\n",
      "Processing: BA\n",
      "Processing: TMO\n",
      "Processing: SPGI\n",
      "Processing: TXN\n",
      "Processing: ISRG\n",
      "Processing: C\n",
      "Processing: MUFG\n",
      "Processing: PDD\n",
      "Processing: QCOM\n",
      "Processing: SONY\n",
      "Processing: AMGN\n",
      "Processing: BSX\n",
      "Processing: ACN\n",
      "Processing: UL\n",
      "Processing: NEE\n",
      "Processing: APP\n",
      "Processing: TJX\n",
      "Processing: AMAT\n",
      "Processing: SYK\n",
      "Processing: PGR\n",
      "Processing: ARM\n",
      "Processing: ADBE\n",
      "Processing: DHR\n",
      "Processing: SPOT\n",
      "Processing: ETN\n",
      "Processing: PFE\n",
      "Processing: HON\n",
      "Processing: DE\n",
      "Processing: GILD\n",
      "Processing: SAN\n",
      "Processing: TTE\n",
      "Processing: LOW\n",
      "Processing: COF\n",
      "Processing: APH\n",
      "Processing: UNP\n",
      "Processing: BHP\n",
      "Processing: KKR\n",
      "Processing: LRCX\n",
      "Processing: TD\n",
      "Processing: MU\n",
      "Processing: UBS\n",
      "Processing: ADP\n",
      "Processing: BTI\n",
      "Processing: KLAC\n",
      "Processing: BUD\n",
      "Processing: MELI\n",
      "Processing: IBN\n",
      "Processing: CMCSA\n",
      "Processing: MDT\n",
      "Processing: COP\n",
      "Processing: DASH\n",
      "Processing: SNPS\n",
      "Processing: SNY\n",
      "Processing: MSTR\n",
      "Processing: WELL\n",
      "Processing: PANW\n",
      "Processing: ADI\n",
      "Processing: NKE\n",
      "Processing: ICE\n",
      "Processing: CB\n",
      "Processing: MO\n",
      "Processing: CRWD\n",
      "Processing: CEG\n",
      "Processing: SO\n",
      "Processing: BBVA\n",
      "Processing: SBUX\n",
      "Processing: RIO\n",
      "Processing: ENB\n",
      "Processing: CME\n",
      "Processing: LMT\n",
      "Processing: PLD\n",
      "Processing: SMFG\n",
      "Processing: MMC\n",
      "Processing: BN\n",
      "Processing: HOOD\n",
      "Processing: AMT\n",
      "Processing: BAM\n",
      "Processing: DUK\n",
      "Processing: CDNS\n",
      "Processing: VRTX\n",
      "Processing: TT\n",
      "Processing: WM\n",
      "Processing: PH\n",
      "Processing: BMY\n",
      "Processing: MCO\n",
      "Processing: DELL\n",
      "Processing: CTAS\n",
      "Processing: RELX\n",
      "Processing: RBLX\n",
      "Processing: BP\n",
      "Processing: SE\n",
      "Processing: ORLY\n",
      "Processing: HCA\n",
      "Processing: SHW\n",
      "Processing: INTC\n",
      "Processing: RCL\n",
      "Processing: NOC\n",
      "Processing: GD\n",
      "Processing: NTES\n",
      "Processing: MCK\n",
      "Processing: TRI\n",
      "Processing: BMO\n",
      "Processing: MMM\n",
      "Processing: PBR\n",
      "Processing: PBR-A\n",
      "Processing: MDLZ\n",
      "Processing: CVS\n",
      "Processing: TDG\n",
      "Processing: COIN\n",
      "Processing: APO\n",
      "Processing: APO-PRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: \n",
      "$APO-PRA: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] APO-PRA: 'Dividends'\n",
      "Processing: RACE\n",
      "Processing: AON\n",
      "Processing: SCCO\n",
      "Processing: CVNA\n",
      "Processing: MFG\n",
      "Processing: ECL\n",
      "Processing: EQIX\n",
      "Processing: NEM\n",
      "Processing: ITW\n",
      "Processing: GSK\n",
      "Processing: EMR\n",
      "Processing: AJG\n",
      "Processing: MSI\n",
      "Processing: PNC\n",
      "Processing: RSG\n",
      "Processing: ABNB\n",
      "Processing: NET\n",
      "Processing: UPS\n",
      "Processing: HWM\n",
      "✅ Volume, MarketCap, and DividendYield data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "TICKER_FILE = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Top200_StockAnalysis.xlsx\"\n",
    "TICKER_COLUMN = \"Ticker\"\n",
    "\n",
    "SAVE_DIR = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "tickers = pd.read_excel(TICKER_FILE)[TICKER_COLUMN].dropna().unique().tolist()\n",
    "\n",
    "volume_data = []\n",
    "marketcap_data = []\n",
    "dividendyield_data = []\n",
    "\n",
    "def get_historical_data(ticker, start='2010-01-01', end='2023-12-31'):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(start=start, end=end, actions=True)\n",
    "\n",
    "        shares_outstanding = stock.info.get('sharesOutstanding', None)\n",
    "        hist['MarketCap'] = hist['Close'] * shares_outstanding if shares_outstanding else None\n",
    "\n",
    "        hist['DividendYield'] = 0.0\n",
    "        for day in hist[hist['Dividends'] > 0].index:\n",
    "            price = hist.at[day, 'Close']\n",
    "            div = hist.at[day, 'Dividends']\n",
    "            hist.at[day, 'DividendYield'] = div / price if price else 0.0\n",
    "\n",
    "        yearly = hist.resample('YE').agg({\n",
    "            'Volume': 'mean',\n",
    "            'MarketCap': 'mean',\n",
    "            'DividendYield': 'mean'\n",
    "        })\n",
    "\n",
    "        yearly['Year'] = yearly.index.year\n",
    "        yearly.reset_index(drop=True, inplace=True)\n",
    "        yearly['Ticker'] = ticker\n",
    "\n",
    "        volume_data.append(yearly[['Ticker', 'Year', 'Volume']])\n",
    "        marketcap_data.append(yearly[['Ticker', 'Year', 'MarketCap']])\n",
    "        dividendyield_data.append(yearly[['Ticker', 'Year', 'DividendYield']])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {ticker}: {e}\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing: {ticker}\")\n",
    "    get_historical_data(ticker)\n",
    "    time.sleep(1)\n",
    "\n",
    "pd.concat(volume_data).to_csv(os.path.join(SAVE_DIR, 'Volume_Data.csv'), index=False)\n",
    "pd.concat(marketcap_data).to_csv(os.path.join(SAVE_DIR, 'MarketCap_Data.csv'), index=False)\n",
    "pd.concat(dividendyield_data).to_csv(os.path.join(SAVE_DIR, 'DividendYield_Data.csv'), index=False)\n",
    "\n",
    "print(\"✅ Volume, MarketCap, and DividendYield data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1587bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TICKER_FILE_PATH = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Top200_StockAnalysis.xlsx\"\n",
    "TICKER_COLUMN = \"Ticker\"\n",
    "DATA_FOLDER = r\"C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6366165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_table(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = bs4.BeautifulSoup(resp.text, \"html.parser\")\n",
    "        tables = soup.find_all(\"table\")\n",
    "        if not tables:\n",
    "            raise ValueError(\"No tables found on the page.\")\n",
    "        df = pd.read_html(StringIO(str(tables[0])))[0]\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "        df.columns = [col.strip() for col in df.columns] \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch data from {url}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf2dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickers_df = pd.read_excel(TICKER_FILE_PATH)\n",
    "tickers_list = tickers_df[TICKER_COLUMN].dropna().unique().tolist()\n",
    "\n",
    "metrics_info = {\n",
    "    'PE': {\n",
    "        'url_suffix': 'pe-ratio',\n",
    "        'expected_cols': {'Date', 'Stock Price', 'TTM Net EPS', 'PE Ratio'},\n",
    "        'rename_map': {'Date': 'Date', 'Stock Price': 'StockPrice', 'TTM Net EPS': 'NetEPS', 'PE Ratio': 'PE'},\n",
    "        'save_file': 'pe_ratio_data.csv'\n",
    "    },\n",
    "    'PB': {\n",
    "        'url_suffix': 'price-book',\n",
    "        'expected_cols': {'Date', 'Stock Price', 'Book Value per Share', 'Price to Book Ratio'},\n",
    "        'rename_map': {'Date': 'Date', 'Stock Price': 'StockPrice', 'Book Value per Share': 'BookValuePerShare', 'Price to Book Ratio': 'PB'},\n",
    "        'save_file': 'pb_ratio_data.csv'\n",
    "    },\n",
    "    'ROE': {\n",
    "        'url_suffix': 'roe',\n",
    "        'expected_cols': {'Date', 'Return on Equity', 'TTM Net Income'},\n",
    "        'rename_map': {'Date': 'Date', 'Return on Equity': 'ROE', 'TTM Net Income': 'NetIncome'},\n",
    "        'save_file': 'roe_data.csv'\n",
    "    },\n",
    "    'ROA': {\n",
    "        'url_suffix': 'roa',\n",
    "        'expected_cols': {'Date', 'Return on Assets', 'Total Assets'},\n",
    "        'rename_map': {'Date': 'Date', 'Return on Assets': 'ROA', 'Total Assets': 'TotalAssets'},\n",
    "        'save_file': 'roa_data.csv'\n",
    "    },\n",
    "    'DebtToEquity': {\n",
    "        'url_suffix': 'debt-equity-ratio',\n",
    "        'expected_cols': {'Date', 'Debt to Equity Ratio'},\n",
    "        'rename_map': {'Date': 'Date', 'Debt to Equity Ratio': 'DebtToEquity'},\n",
    "        'save_file': 'debt_to_equity_data.csv'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c1ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/200: NVDA (nvda)\n",
      "Processing 2/200: MSFT (msft)\n",
      "Processing 3/200: AAPL (aapl)\n",
      "Processing 4/200: AMZN (amzn)\n",
      "Processing 5/200: GOOGL (googl)\n",
      "Processing 6/200: META (meta)\n",
      "Processing 7/200: AVGO (avgo)\n",
      "Processing 8/200: TSLA (tsla)\n",
      "Processing 9/200: TSM (tsm)\n",
      "Processing 10/200: BRK.B (brk.b)\n",
      "Processing 11/200: WMT (wmt)\n",
      "Processing 12/200: JPM (jpm)\n",
      "Processing 13/200: ORCL (orcl)\n",
      "Processing 14/200: V (v)\n",
      "Processing 15/200: LLY (lly)\n",
      "Processing 16/200: MA (ma)\n",
      "Processing 17/200: NFLX (nflx)\n",
      "[ERROR] Failed to fetch data from https://www.macrotrends.net/stocks/charts/NFLX/nflx/roa: HTTPSConnectionPool(host='www.macrotrends.net', port=443): Read timed out. (read timeout=10)\n",
      "⚠️ No data found for ROA of NFLX\n",
      "Processing 18/200: XOM (xom)\n",
      "Processing 19/200: COST (cost)\n",
      "Processing 20/200: PLTR (pltr)\n",
      "Processing 21/200: JNJ (jnj)\n",
      "[ERROR] Failed to fetch data from https://www.macrotrends.net/stocks/charts/JNJ/jnj/roe: HTTPSConnectionPool(host='www.macrotrends.net', port=443): Read timed out.\n",
      "⚠️ No data found for ROE of JNJ\n",
      "Processing 22/200: HD (hd)\n",
      "Processing 23/200: PG (pg)\n",
      "Processing 24/200: ABBV (abbv)\n",
      "Processing 25/200: SAP (sap)\n",
      "Processing 26/200: BAC (bac)\n",
      "Processing 27/200: CVX (cvx)\n",
      "[ERROR] Failed to fetch data from https://www.macrotrends.net/stocks/charts/CVX/cvx/price-book: HTTPSConnectionPool(host='www.macrotrends.net', port=443): Read timed out. (read timeout=10)\n",
      "⚠️ No data found for PB of CVX\n",
      "Processing 28/200: KO (ko)\n",
      "Processing 29/200: GE (ge)\n",
      "Processing 30/200: AMD (amd)\n",
      "Processing 31/200: CSCO (csco)\n",
      "Processing 32/200: ASML (asml)\n",
      "Processing 33/200: TMUS (tmus)\n",
      "Processing 34/200: BABA (baba)\n",
      "Processing 35/200: PM (pm)\n",
      "Processing 36/200: WFC (wfc)\n",
      "Processing 37/200: TM (tm)\n",
      "Processing 38/200: IBM (ibm)\n",
      "Processing 39/200: CRM (crm)\n",
      "Processing 40/200: ABT (abt)\n",
      "Processing 41/200: AZN (azn)\n",
      "Processing 42/200: MS (ms)\n",
      "Processing 43/200: NVS (nvs)\n",
      "Processing 44/200: UNH (unh)\n",
      "Processing 45/200: LIN (lin)\n",
      "Processing 46/200: MCD (mcd)\n",
      "Processing 47/200: HSBC (hsbc)\n",
      "Processing 48/200: NVO (nvo)\n",
      "Processing 49/200: INTU (intu)\n",
      "Processing 50/200: SHEL (shel)\n",
      "Processing 51/200: RTX (rtx)\n",
      "Processing 52/200: GS (gs)\n",
      "Processing 53/200: BX (bx)\n",
      "Processing 54/200: AXP (axp)\n",
      "Processing 55/200: DIS (dis)\n",
      "Processing 56/200: MRK (mrk)\n",
      "Processing 57/200: T (t)\n",
      "Processing 58/200: PEP (pep)\n",
      "Processing 59/200: SHOP (shop)\n",
      "Processing 60/200: CAT (cat)\n",
      "Processing 61/200: UBER (uber)\n",
      "Processing 62/200: RY (ry)\n",
      "Processing 63/200: NOW (now)\n",
      "Processing 64/200: VZ (vz)\n",
      "Processing 65/200: BKNG (bkng)\n",
      "Processing 66/200: GEV (gev)\n",
      "Processing 67/200: HDB (hdb)\n",
      "Processing 68/200: ANET (anet)\n",
      "Processing 69/200: SCHW (schw)\n",
      "Processing 70/200: BLK (blk)\n",
      "Processing 71/200: BA (ba)\n",
      "Processing 72/200: TMO (tmo)\n",
      "Processing 73/200: SPGI (spgi)\n",
      "Processing 74/200: TXN (txn)\n",
      "Processing 75/200: ISRG (isrg)\n",
      "Processing 76/200: C (c)\n",
      "Processing 77/200: MUFG (mufg)\n",
      "Processing 78/200: PDD (pdd)\n",
      "Processing 79/200: QCOM (qcom)\n",
      "Processing 80/200: SONY (sony)\n",
      "Processing 81/200: AMGN (amgn)\n",
      "Processing 82/200: BSX (bsx)\n",
      "Processing 83/200: ACN (acn)\n",
      "Processing 84/200: UL (ul)\n",
      "Processing 85/200: NEE (nee)\n",
      "Processing 86/200: APP (app)\n",
      "Processing 87/200: TJX (tjx)\n",
      "Processing 88/200: AMAT (amat)\n",
      "Processing 89/200: SYK (syk)\n",
      "Processing 90/200: PGR (pgr)\n",
      "Processing 91/200: ARM (arm)\n",
      "Processing 92/200: ADBE (adbe)\n",
      "Processing 93/200: DHR (dhr)\n",
      "Processing 94/200: SPOT (spot)\n",
      "Processing 95/200: ETN (etn)\n",
      "Processing 96/200: PFE (pfe)\n",
      "Processing 97/200: HON (hon)\n",
      "Processing 98/200: DE (de)\n",
      "Processing 99/200: GILD (gild)\n",
      "Processing 100/200: SAN (san)\n",
      "Processing 101/200: TTE (tte)\n",
      "Processing 102/200: LOW (low)\n",
      "Processing 103/200: COF (cof)\n",
      "Processing 104/200: APH (aph)\n",
      "Processing 105/200: UNP (unp)\n",
      "Processing 106/200: BHP (bhp)\n",
      "⚠️ No data found for ROE of BHP\n",
      "⚠️ No data found for ROA of BHP\n",
      "Processing 107/200: KKR (kkr)\n",
      "Processing 108/200: LRCX (lrcx)\n",
      "Processing 109/200: TD (td)\n",
      "Processing 110/200: MU (mu)\n",
      "Processing 111/200: UBS (ubs)\n",
      "Processing 112/200: ADP (adp)\n",
      "Processing 113/200: BTI (bti)\n",
      "⚠️ No data found for ROE of BTI\n",
      "⚠️ No data found for ROA of BTI\n",
      "Processing 114/200: KLAC (klac)\n",
      "Processing 115/200: BUD (bud)\n",
      "Processing 116/200: MELI (meli)\n",
      "Processing 117/200: IBN (ibn)\n",
      "Processing 118/200: CMCSA (cmcsa)\n",
      "Processing 119/200: MDT (mdt)\n",
      "Processing 120/200: COP (cop)\n",
      "Processing 121/200: DASH (dash)\n",
      "Processing 122/200: SNPS (snps)\n",
      "Processing 123/200: SNY (sny)\n",
      "Processing 124/200: MSTR (mstr)\n",
      "Processing 125/200: WELL (well)\n",
      "Processing 126/200: PANW (panw)\n",
      "Processing 127/200: ADI (adi)\n",
      "Processing 128/200: NKE (nke)\n",
      "Processing 129/200: ICE (ice)\n",
      "Processing 130/200: CB (cb)\n",
      "Processing 131/200: MO (mo)\n",
      "Processing 132/200: CRWD (crwd)\n",
      "Processing 133/200: CEG (ceg)\n",
      "Processing 134/200: SO (so)\n",
      "Processing 135/200: BBVA (bbva)\n",
      "Processing 136/200: SBUX (sbux)\n",
      "Processing 137/200: RIO (rio)\n",
      "⚠️ No data found for ROE of RIO\n",
      "⚠️ No data found for ROA of RIO\n",
      "Processing 138/200: ENB (enb)\n",
      "Processing 139/200: CME (cme)\n",
      "Processing 140/200: LMT (lmt)\n",
      "Processing 141/200: PLD (pld)\n",
      "Processing 142/200: SMFG (smfg)\n",
      "Processing 143/200: MMC (mmc)\n",
      "Processing 144/200: BN (bn)\n",
      "Processing 145/200: HOOD (hood)\n",
      "Processing 146/200: AMT (amt)\n",
      "Processing 147/200: BAM (bam)\n",
      "Processing 148/200: DUK (duk)\n",
      "Processing 149/200: CDNS (cdns)\n",
      "Processing 150/200: VRTX (vrtx)\n",
      "Processing 151/200: TT (tt)\n",
      "Processing 152/200: WM (wm)\n",
      "Processing 153/200: PH (ph)\n",
      "Processing 154/200: BMY (bmy)\n",
      "Processing 155/200: MCO (mco)\n",
      "Processing 156/200: DELL (dell)\n",
      "Processing 157/200: CTAS (ctas)\n",
      "Processing 158/200: RELX (relx)\n",
      "⚠️ No data found for ROE of RELX\n",
      "⚠️ No data found for ROA of RELX\n",
      "Processing 159/200: RBLX (rblx)\n",
      "Processing 160/200: BP (bp)\n",
      "Processing 161/200: SE (se)\n",
      "Processing 162/200: ORLY (orly)\n",
      "Processing 163/200: HCA (hca)\n",
      "Processing 164/200: SHW (shw)\n",
      "Processing 165/200: INTC (intc)\n",
      "Processing 166/200: RCL (rcl)\n",
      "Processing 167/200: NOC (noc)\n",
      "Processing 168/200: GD (gd)\n",
      "Processing 169/200: NTES (ntes)\n",
      "Processing 170/200: MCK (mck)\n",
      "Processing 171/200: TRI (tri)\n",
      "Processing 172/200: BMO (bmo)\n",
      "Processing 173/200: MMM (mmm)\n",
      "Processing 174/200: PBR (pbr)\n",
      "Processing 175/200: PBR.A (pbr.a)\n",
      "Processing 176/200: MDLZ (mdlz)\n",
      "Processing 177/200: CVS (cvs)\n",
      "Processing 178/200: TDG (tdg)\n",
      "Processing 179/200: COIN (coin)\n",
      "Processing 180/200: APO (apo)\n",
      "Processing 181/200: APO.PRA (apo.pra)\n",
      "⚠️ No data found for ROE of APO.PRA\n",
      "⚠️ No data found for ROA of APO.PRA\n",
      "⚠️ No data found for DebtToEquity of APO.PRA\n",
      "Processing 182/200: RACE (race)\n",
      "Processing 183/200: AON (aon)\n",
      "Processing 184/200: SCCO (scco)\n",
      "Processing 185/200: CVNA (cvna)\n",
      "Processing 186/200: MFG (mfg)\n",
      "Processing 187/200: ECL (ecl)\n",
      "Processing 188/200: EQIX (eqix)\n",
      "Processing 189/200: NEM (nem)\n",
      "Processing 190/200: ITW (itw)\n",
      "Processing 191/200: GSK (gsk)\n",
      "Processing 192/200: EMR (emr)\n",
      "Processing 193/200: AJG (ajg)\n",
      "Processing 194/200: MSI (msi)\n",
      "Processing 195/200: PNC (pnc)\n",
      "Processing 196/200: RSG (rsg)\n",
      "Processing 197/200: ABNB (abnb)\n",
      "Processing 198/200: NET (net)\n",
      "Processing 199/200: UPS (ups)\n",
      "Processing 200/200: HWM (hwm)\n",
      "\n",
      "✅ All scraping complete. Saving files...\n",
      "\n",
      "✅ Saved PE data to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\\PE_Annual.csv\n",
      "✅ Saved PB data to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\\PB_Annual.csv\n",
      "✅ Saved ROE data to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\\ROE_Annual.csv\n",
      "✅ Saved ROA data to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\\ROA_Annual.csv\n",
      "✅ Saved DebtToEquity data to: C:\\Users\\surji\\Desktop\\Quant_Poject\\Complete_analysis_200stocks\\Data\\Files\\DebtToEquity_Annual.csv\n"
     ]
    }
   ],
   "source": [
    "metric_data = {key: [] for key in metrics_info.keys()}\n",
    "\n",
    "# Start scraping\n",
    "for idx, ticker in enumerate(tickers_list):\n",
    "    company_name = ticker.lower()\n",
    "    print(f\"Processing {idx+1}/{len(tickers_list)}: {ticker} ({company_name})\")\n",
    "\n",
    "    for metric_key, info in metrics_info.items():\n",
    "        url = f\"https://www.macrotrends.net/stocks/charts/{ticker}/{company_name}/{info['url_suffix']}\"\n",
    "        df_metric = fetch_table(url)\n",
    "\n",
    "        if df_metric is not None and not df_metric.empty:\n",
    "            if not info['expected_cols'].issubset(df_metric.columns):\n",
    "                print(f\"⚠️ Warning: Expected columns missing for {metric_key} of {ticker}\")\n",
    "            else:\n",
    "                try:\n",
    "                    df_metric = df_metric.rename(columns=info['rename_map'])\n",
    "                    df_metric['Date'] = pd.to_datetime(df_metric['Date'], errors='coerce')\n",
    "                    df_metric = df_metric[df_metric['Date'].between('2010-01-01', '2025-12-31')]\n",
    "\n",
    "                    # Clean and convert metric columns to numeric\n",
    "                    agg_columns = list(info['rename_map'].values())[1:]  # skip 'Date'\n",
    "                    for col in agg_columns:\n",
    "                        df_metric[col] = (\n",
    "                            df_metric[col]\n",
    "                            .astype(str)\n",
    "                            .str.replace(',', '')\n",
    "                            .str.replace('%', '')\n",
    "                        )\n",
    "                        df_metric[col] = pd.to_numeric(df_metric[col], errors='coerce')\n",
    "\n",
    "                    df_metric['Ticker'] = ticker\n",
    "                    df_metric['Year'] = df_metric['Date'].dt.year\n",
    "\n",
    "                    # Aggregate quarterly → annual\n",
    "                    df_annual = df_metric.groupby(['Ticker', 'Year'], as_index=False)[agg_columns].mean()\n",
    "\n",
    "                    # Rename 'Year' → 'Date' and convert to datetime\n",
    "                    df_annual = df_annual.rename(columns={'Year': 'Date'})\n",
    "                    df_annual['Date'] = pd.to_datetime(df_annual['Date'], format='%Y')\n",
    "\n",
    "                    metric_data[metric_key].append(df_annual)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing {metric_key} data for {ticker}: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No data found for {metric_key} of {ticker}\")\n",
    "\n",
    "        time.sleep(random.uniform(15, 30))  # Respectful scraping delay\n",
    "\n",
    "# Save all processed metrics\n",
    "print(\"\\n✅ All scraping complete. Saving files...\\n\")\n",
    "\n",
    "for metric_key, df_list in metric_data.items():\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        output_path = os.path.join(DATA_FOLDER, f\"{metric_key}_Annual.csv\")\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "        print(f\"✅ Saved {metric_key} data to: {output_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No data to save for {metric_key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peratio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
